{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Perceptron for XOR.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM0MKzyVEv4kY9Nn/1HcBbJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swara-24/AI-2020/blob/master/Perceptron_for_XOR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWRKXHOSR0v9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "bbcc1208-5f7d-407f-fb0f-0a1be1caa0d5"
      },
      "source": [
        "\n",
        "import numpy as np \n",
        "def unitStep(v): \n",
        "\tif v >= 0: \n",
        "\t\treturn 1\n",
        "\telse: \n",
        "\t\treturn 0\n",
        "def perceptronModel(x, w, b): \n",
        "\tv = np.dot(w, x) + b \n",
        "\ty = unitStep(v) \n",
        "\treturn y \n",
        "def NOT_logicFunction(x): \n",
        "\twNOT = -1\n",
        "\tbNOT = 0.5\n",
        "\treturn perceptronModel(x, wNOT, bNOT) \n",
        "def AND_logicFunction(x): \n",
        "\tw = np.array([1, 1]) \n",
        "\tbAND = -1.5\n",
        "\treturn perceptronModel(x, w, bAND) \n",
        "def OR_logicFunction(x): \n",
        "\tw = np.array([1, 1]) \n",
        "\tbOR = -0.5\n",
        "\treturn perceptronModel(x, w, bOR) \n",
        "def XOR_logicFunction(x): \n",
        "\ty1 = AND_logicFunction(x) \n",
        "\ty2 = OR_logicFunction(x) \n",
        "\ty3 = NOT_logicFunction(y1) \n",
        "\tfinal_x = np.array([y2, y3]) \n",
        "\tfinalOutput = AND_logicFunction(final_x) \n",
        "\treturn finalOutput \n",
        "\n",
        "# testing the Perceptron Model \n",
        "test1 = np.array([0, 1]) \n",
        "test2 = np.array([1, 1]) \n",
        "test3 = np.array([0, 0]) \n",
        "test4 = np.array([1, 0]) \n",
        "\n",
        "print(\"XOR({}, {}) = {}\".format(0, 1, XOR_logicFunction(test1))) \n",
        "print(\"XOR({}, {}) = {}\".format(1, 1, XOR_logicFunction(test2))) \n",
        "print(\"XOR({}, {}) = {}\".format(0, 0, XOR_logicFunction(test3))) \n",
        "print(\"XOR({}, {}) = {}\".format(1, 0, XOR_logicFunction(test4))) \n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "XOR(0, 1) = 1\n",
            "XOR(1, 1) = 0\n",
            "XOR(0, 0) = 0\n",
            "XOR(1, 0) = 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1K3zQKpuS2Yc",
        "colab_type": "text"
      },
      "source": [
        "Here, the model predicted output ($\\boldsymbol{\\hat{y}}$) for each of the inputs are exactly matched with the XOR logic gate output ($\\boldsymbol{y}$) according to the truth table.\n",
        "Hence, it is verified that the perceptron algorithm for XOR logic gate is correctly implemented."
      ]
    }
  ]
}